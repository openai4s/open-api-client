openapi: 3.0.0
info:
  title: OpenAI API
  description: APIs for sampling from and fine-tuning language models
  version: '1.2.0'
servers:
  - url: https://api.openai.com/v1
tags:
  - name: OpenAI
    description: The OpenAI REST API
paths:

  /chat/completions:
    post:
      operationId: createChatCompletion
      tags:
        - OpenAI
      summary: Creates a completion for the chat message
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateChatCompletionRequest'
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionResponse'

      x-oaiMeta:
        name: Create chat completion
        group: chat
        path: create
        beta: true
        examples:
          curl: |
            curl https://api.openai.com/v1/chat/completions \
              -H 'Content-Type: application/json' \
              -H 'Authorization: Bearer YOUR_API_KEY' \
              -d '{
              "model": "gpt-3.5-turbo",
              "messages": [{"role": "user", "content": "Hello!"}]
            }'
          python: |
            import os
            import openai
            openai.api_key = os.getenv("OPENAI_API_KEY")

            completion = openai.ChatCompletion.create(
              model="gpt-3.5-turbo",
              messages=[
                {"role": "user", "content": "Hello!"}
              ]
            )

            print(completion.choices[0].message)
          node.js: |
            const { Configuration, OpenAIApi } = require("openai");

            const configuration = new Configuration({
              apiKey: process.env.OPENAI_API_KEY,
            });
            const openai = new OpenAIApi(configuration);

            const completion = await openai.createChatCompletion({
              model: "gpt-3.5-turbo",
              messages: [{role: "user", content: "Hello world"}],
            });
            console.log(completion.data.choices[0].message);
        parameters: |
          {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": "Hello!"}]
          }
        response: |
          {
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677652288,
            "choices": [{
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "\n\nHello there, how may I assist you today?",
              },
              "finish_reason": "stop"
            }],
            "usage": {
              "prompt_tokens": 9,
              "completion_tokens": 12,
              "total_tokens": 21
            }
          }


components:
  schemas:

    ChatCompletionRequestMessage:
      type: object
      properties:
        role:
          type: string
          enum: ["system", "user", "assistant"]
          description: The role of the author of this message.
        content:
          type: string
          description: The contents of the message
        name:
          type: string
          description: The name of the user in a multi-user chat
      required:
        - role
        - content

    ChatCompletionResponseMessage:
      type: object
      properties:
        role:
          type: string
          enum: ["system", "user", "assistant"]
          description: The role of the author of this message.
        content:
          type: string
          description: The contents of the message
      required:
        - role
        - content

    CreateChatCompletionRequest:
      type: object
      properties:
        model:
          description: ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.
          type: string
        messages:
          description: The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction).
          type: array
          minItems: 1
          items:
            $ref: '#/components/schemas/ChatCompletionRequestMessage'
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          required: false
          description: &completions_temperature_description |
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
            We generally recommend altering this or `top_p` but not both.

        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          required: false
          description: &completions_top_p_description |
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
            We generally recommend altering this or `temperature` but not both.
        n:
          type: integer
          minimum: 1
          maximum: 128
          default: 1
          example: 1
          required: false
          description: How many chat completion choices to generate for each input message.
        stream:
          description: >
            If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data: [DONE]` message.
          type: boolean
          required: false
          default: false
        stop:
          description: &completions_stop_description >
            Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
          required: false
          x-scala-type: io.github.openai4s.codecs.Stop
          type: string
#        stop:
#          description: |
#            Up to 4 sequences where the API will stop generating further tokens.
#          default: null
#          oneOf:
#            - type: string
#              required: false
#            - type: array
#              minItems: 1
#              maxItems: 4
#              items:
#                type: string
        max_tokens:
          description: |
            The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).
          default: inf
          type: integer
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          required: false
          description:  &completions_presence_penalty_description |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.

            [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          required: false
          description: &completions_frequency_penalty_description |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

            [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
        logit_bias:
          type: object
          x-oaiTypeLabel: map
          required: false
          description: |
            Modify the likelihood of specified tokens appearing in the completion.

            Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
        user: &end_user_param_configuration
          type: string
          example: user-1234
          description: |
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
      required:
        - model
        - messages

    CreateChatCompletionResponse:
      type: object
      properties:
        id:
          type: string
        object:
          type: string
        created:
          type: integer
        model:
          type: string
        choices:
          type: array
          items:
            type: object
            properties:
              index:
                type: integer
              message:
                $ref: '#/components/schemas/ChatCompletionResponseMessage'
              finish_reason:
                type: string
        usage:
          type: object
          properties:
            prompt_tokens:
              type: integer
            completion_tokens:
              type: integer
            total_tokens:
              type: integer
          required:
            - prompt_tokens
            - completion_tokens
            - total_tokens
      required:
        - id
        - object
        - created
        - model
        - choices

x-oaiMeta:
  groups:
    - id: models
      title: Models
      description: |
        List and describe the various models available in the API. You can refer to the [Models](/docs/models) documentation to understand what models are available and the differences between them.
    - id: completions
      title: Completions
      description: |
        Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.
    - id: chat
      title: Chat
      description: |
        Given a chat conversation, the model will return a chat completion response.
    - id: edits
      title: Edits
      description: |
        Given a prompt and an instruction, the model will return an edited version of the prompt.
    - id: images
      title: Images
      description: |
        Given a prompt and/or an input image, the model will generate a new image.

        Related guide: [Image generation](/docs/guides/images)
    - id: embeddings
      title: Embeddings
      description: |
        Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.

        Related guide: [Embeddings](/docs/guides/embeddings)
    - id: audio
      title: Audio
      description: |
        Learn how to turn audio into text.

        Related guide: [Speech to text](/docs/guides/speech-to-text)
    - id: files
      title: Files
      description: |
        Files are used to upload documents that can be used with features like [Fine-tuning](/docs/api-reference/fine-tunes).
    - id: fine-tunes
      title: Fine-tunes
      description: |
        Manage fine-tuning jobs to tailor a model to your specific training data.

        Related guide: [Fine-tune models](/docs/guides/fine-tuning)
    - id: moderations
      title: Moderations
      description: |
        Given a input text, outputs if the model classifies it as violating OpenAI's content policy.

        Related guide: [Moderations](/docs/guides/moderation)
    - id: searches
      title: Searches
      warning:
        title: This endpoint is deprecated and will be removed on December 3rd, 2022
        message: We’ve developed new methods with better performance. [Learn more](https://help.openai.com/en/articles/6272952-search-transition-guide).
      description: |
        Given a query and a set of documents or labels, the model ranks each document based on its semantic similarity to the provided query.

        Related guide: [Search](/docs/guides/search)
    - id: classifications
      title: Classifications
      warning:
        title: This endpoint is deprecated and will be removed on December 3rd, 2022
        message: We’ve developed new methods with better performance. [Learn more](https://help.openai.com/en/articles/6272941-classifications-transition-guide).
      description: |
        Given a query and a set of labeled examples, the model will predict the most likely label for the query. Useful as a drop-in replacement for any ML classification or text-to-label task.

        Related guide: [Classification](/docs/guides/classifications)
    - id: answers
      title: Answers
      warning:
        title: This endpoint is deprecated and will be removed on December 3rd, 2022
        message: We’ve developed new methods with better performance. [Learn more](https://help.openai.com/en/articles/6233728-answers-transition-guide).
      description: |
        Given a question, a set of documents, and some examples, the API generates an answer to the question based on the information in the set of documents. This is useful for question-answering applications on sources of truth, like company documentation or a knowledge base.

        Related guide: [Question answering](/docs/guides/answers)
    - id: engines
      title: Engines
      description: These endpoints describe and provide access to the various engines available in the API.
      warning:
        title: The Engines endpoints are deprecated.
        message: Please use their replacement, [Models](/docs/api-reference/models), instead. [Learn more](https://help.openai.com/TODO).
