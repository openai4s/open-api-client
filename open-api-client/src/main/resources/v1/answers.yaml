openapi: 3.0.0
info:
  title: OpenAI API
  description: APIs for sampling from and fine-tuning language models
  version: '1.2.0'
servers:
  - url: https://api.openai.com/v1
tags:
  - name: OpenAI
    description: The OpenAI REST API
paths:

  /answers:
    post:
      operationId: createAnswer
      deprecated: true
      tags:
        - OpenAI
      summary: |
        Answers the specified question using the provided documents and examples.

        The endpoint first [searches](/docs/api-reference/searches) over provided documents or files to find relevant context. The relevant context is combined with the provided examples and question to create the prompt for [completion](/docs/api-reference/completions).
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateAnswerRequest'
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateAnswerResponse'
      x-oaiMeta:
        name: Create answer
        group: answers
        path: create
        examples:
          curl: |
            curl https://api.openai.com/v1/answers \
              -X POST \
              -H "Authorization: Bearer YOUR_API_KEY" \
              -H 'Content-Type: application/json' \
              -d '{
                "documents": ["Puppy A is happy.", "Puppy B is sad."],
                "question": "which puppy is happy?",
                "search_model": "ada",
                "model": "curie",
                "examples_context": "In 2017, U.S. life expectancy was 78.6 years.",
                "examples": [["What is human life expectancy in the United States?","78 years."]],
                "max_tokens": 5,
                "stop": ["\n", "<|endoftext|>"]
              }'

          python: |
            import os
            import openai
            openai.api_key = os.getenv("OPENAI_API_KEY")
            openai.Answer.create(
              search_model="ada",
              model="curie",
              question="which puppy is happy?",
              documents=["Puppy A is happy.", "Puppy B is sad."],
              examples_context="In 2017, U.S. life expectancy was 78.6 years.",
              examples=[["What is human life expectancy in the United States?","78 years."]],
              max_tokens=5,
              stop=["\n", "<|endoftext|>"],
            )
          node.js: |
            const { Configuration, OpenAIApi } = require("openai");
            const configuration = new Configuration({
              apiKey: process.env.OPENAI_API_KEY,
            });
            const openai = new OpenAIApi(configuration);
            const response = await openai.createAnswer({
              search_model: "ada",
              model: "curie",
              question: "which puppy is happy?",
              documents: ["Puppy A is happy.", "Puppy B is sad."],
              examples_context: "In 2017, U.S. life expectancy was 78.6 years.",
              examples: [["What is human life expectancy in the United States?","78 years."]],
              max_tokens: 5,
              stop: ["\n", "<|endoftext|>"],
            });
        parameters: |
          {
            "documents": ["Puppy A is happy.", "Puppy B is sad."],
            "question": "which puppy is happy?",
            "search_model": "ada",
            "model": "curie",
            "examples_context": "In 2017, U.S. life expectancy was 78.6 years.",
            "examples": [["What is human life expectancy in the United States?","78 years."]],
            "max_tokens": 5,
            "stop": ["\n", "<|endoftext|>"]
          }
        response: |
          {
            "answers": [
              "puppy A."
            ],
            "completion": "cmpl-2euVa1kmKUuLpSX600M41125Mo9NI",
            "model": "curie:2020-05-03",
            "object": "answer",
            "search_model": "ada",
            "selected_documents": [
              {
                "document": 0,
                "text": "Puppy A is happy. "
              },
              {
                "document": 1,
                "text": "Puppy B is sad. "
              }
            ]
          }


components:
  schemas:

    CreateAnswerRequest:
      type: object
      additionalProperties: false
      properties:
        model:
          description: ID of the model to use for completion. You can select one of `ada`, `babbage`, `curie`, or `davinci`.
          type: string
        question:
          description: Question to get answered.
          type: string
          minLength: 1
          example: "What is the capital of Japan?"
        examples:
          description: List of (question, answer) pairs that will help steer the model towards the tone and answer format you'd like. We recommend adding 2 to 3 examples.
          type: array
          minItems: 1
          maxItems: 200
          items:
            type: array
            minItems: 2
            maxItems: 2
            items:
              type: string
              minLength: 1
          example: "[['What is the capital of Canada?', 'Ottawa'], ['Which province is Ottawa in?', 'Ontario']]"
        examples_context:
          description: A text snippet containing the contextual information used to generate the answers for the `examples` you provide.
          type: string
          example: "Ottawa, Canada's capital, is located in the east of southern Ontario, near the city of Montréal and the U.S. border."
        documents:
          description: |
            List of documents from which the answer for the input `question` should be derived. If this is an empty list, the question will be answered based on the question-answer examples.

            You should specify either `documents` or a `file`, but not both.
          type: array
          maxItems: 200
          items:
            type: string
          example: "['Japan is an island country in East Asia, located in the northwest Pacific Ocean.', 'Tokyo is the capital and most populous prefecture of Japan.']"
          nullable: true
        file:
          description: |
            The ID of an uploaded file that contains documents to search over. See [upload file](/docs/api-reference/files/upload) for how to upload a file of the desired format and purpose.

            You should specify either `documents` or a `file`, but not both.
          type: string
          nullable: true
        search_model: &search_model_configuration
          description: ID of the model to use for [Search](/docs/api-reference/searches/create). You can select one of `ada`, `babbage`, `curie`, or `davinci`.
          type: string
          default: ada
          nullable: true
        max_rerank:
          description: The maximum number of documents to be ranked by [Search](/docs/api-reference/searches/create) when using `file`. Setting it to a higher value leads to improved accuracy but with increased latency and cost.
          type: integer
          default: 200
          nullable: true
        temperature:
          description: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
          type: number
          default: 0
          nullable: true
        logprobs: &context_completions_logprobs_configuration
          type: integer
          minimum: 0
          maximum: 5
          default: null
          nullable: true
          description: |
            Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.

            The maximum value for `logprobs` is 5. If you need more than this, please contact us through our [Help center](https://help.openai.com) and describe your use case.

            When `logprobs` is set, `completion` will be automatically added into `expand` to get the logprobs.
        max_tokens:
          description: The maximum number of tokens allowed for the generated answer
          type: integer
          default: 16
          nullable: true
        stop:
          description: &completions_stop_description >
            Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
          default: null
          oneOf:
            - type: string
              default: <|endoftext|>
              example: "\n"
            - type: array
              minItems: 1
              maxItems: 4
              items:
                type: string
                example: '["\n"]'
          nullable: true
        n:
          description: How many answers to generate for each question.
          type: integer
          minimum: 1
          maximum: 10
          default: 1
          nullable: true
        logit_bias: &completions_logit_bias
          type: object
          x-oaiTypeLabel: map
          default: null
          nullable: true
          description: &completions_logit_bias_description |
            Modify the likelihood of specified tokens appearing in the completion.
            
            Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
            
            As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.
        return_metadata: &return_metadata_configuration
          description: |
            A special boolean flag for showing metadata. If set to `true`, each document entry in the returned JSON will contain a "metadata" field.

            This flag only takes effect when `file` is set.
          type: boolean
          default: false
          nullable: true
        return_prompt: &return_prompt_configuration
          description: If set to `true`, the returned JSON will include a "prompt" field containing the final prompt that was used to request a completion. This is mainly useful for debugging purposes.
          type: boolean
          default: false
          nullable: true
        expand: &expand_configuration
          description: If an object name is in the list, we provide the full information of the object; otherwise, we only provide the object ID. Currently we support `completion` and `file` objects for expansion.
          type: array
          items:
            type: object
          required: false
          default: []
        user: &end_user_param_configuration
          type: string
          example: user-1234
          description: |
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
      required:
        - model
        - question
        - examples
        - examples_context

    CreateAnswerResponse:
      type: object
      properties:
        object:
          type: string
        model:
          type: string
        search_model:
          type: string
        completion:
          type: string
        answers:
          type: array
          items:
            type: string
        selected_documents:
          type: array
          items:
            type: object
            properties:
              document:
                type: integer
              text:
                type: string

x-oaiMeta:
  groups:
    - id: models
      title: Models
      description: |
        List and describe the various models available in the API. You can refer to the [Models](/docs/models) documentation to understand what models are available and the differences between them.
    - id: completions
      title: Completions
      description: |
        Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.
    - id: chat
      title: Chat
      description: |
        Given a chat conversation, the model will return a chat completion response.
    - id: edits
      title: Edits
      description: |
        Given a prompt and an instruction, the model will return an edited version of the prompt.
    - id: images
      title: Images
      description: |
        Given a prompt and/or an input image, the model will generate a new image.

        Related guide: [Image generation](/docs/guides/images)
    - id: embeddings
      title: Embeddings
      description: |
        Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.

        Related guide: [Embeddings](/docs/guides/embeddings)
    - id: audio
      title: Audio
      description: |
        Learn how to turn audio into text.

        Related guide: [Speech to text](/docs/guides/speech-to-text)
    - id: files
      title: Files
      description: |
        Files are used to upload documents that can be used with features like [Fine-tuning](/docs/api-reference/fine-tunes).
    - id: fine-tunes
      title: Fine-tunes
      description: |
        Manage fine-tuning jobs to tailor a model to your specific training data.

        Related guide: [Fine-tune models](/docs/guides/fine-tuning)
    - id: moderations
      title: Moderations
      description: |
        Given a input text, outputs if the model classifies it as violating OpenAI's content policy.

        Related guide: [Moderations](/docs/guides/moderation)
    - id: searches
      title: Searches
      warning:
        title: This endpoint is deprecated and will be removed on December 3rd, 2022
        message: We’ve developed new methods with better performance. [Learn more](https://help.openai.com/en/articles/6272952-search-transition-guide).
      description: |
        Given a query and a set of documents or labels, the model ranks each document based on its semantic similarity to the provided query.

        Related guide: [Search](/docs/guides/search)
    - id: classifications
      title: Classifications
      warning:
        title: This endpoint is deprecated and will be removed on December 3rd, 2022
        message: We’ve developed new methods with better performance. [Learn more](https://help.openai.com/en/articles/6272941-classifications-transition-guide).
      description: |
        Given a query and a set of labeled examples, the model will predict the most likely label for the query. Useful as a drop-in replacement for any ML classification or text-to-label task.

        Related guide: [Classification](/docs/guides/classifications)
    - id: answers
      title: Answers
      warning:
        title: This endpoint is deprecated and will be removed on December 3rd, 2022
        message: We’ve developed new methods with better performance. [Learn more](https://help.openai.com/en/articles/6233728-answers-transition-guide).
      description: |
        Given a question, a set of documents, and some examples, the API generates an answer to the question based on the information in the set of documents. This is useful for question-answering applications on sources of truth, like company documentation or a knowledge base.

        Related guide: [Question answering](/docs/guides/answers)
    - id: engines
      title: Engines
      description: These endpoints describe and provide access to the various engines available in the API.
      warning:
        title: The Engines endpoints are deprecated.
        message: Please use their replacement, [Models](/docs/api-reference/models), instead. [Learn more](https://help.openai.com/TODO).
